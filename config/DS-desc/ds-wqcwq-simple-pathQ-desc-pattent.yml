# export PYTHONPATH=`pwd` && echo $PYTHONPATH
# python main/run_ds_bart.py -c config/DS-desc/ds-wqcwq-simple-pathQ-desc-pattent.yml

# helper
only_test_mode: false
test_mode: false
debug: false
eval_all: false

# ver + path
dataset: wqcwq
ver: "ds-wqcwQ_simpleQ_pathQ-desc-pattent"
serialization_dir: save/ds-bart-wiki18
predictions_output_file: predictions.json
vocabulary: vocabulary
load_weights_file: weights.th
save_weights_file: weights.th
metrics_output_file: evaluate_metric.json
comments: "占位符是 [entity 1] [entity 2] 新的input格式 训练"
model_name: "pretrained_models/facebook/bart-base"

update_layers: "all"

predict_vers: [""]

# path
# merge-random-full-v1.0.jsonl  or  merge-longest-0.01.jsonl
train_dataset: "datasets/for_pmt/ds-wiki18/wqcwq_pathq_simpleq/wqcwq_pathq_simpleq-pred-filterByWeight-DESC-v1.0.jsonl"
dataset_file: ""
pmt_save_dir: ""

load_model: false

# ds
setting: "ds-merge-desc-pattent"
add_desc: true
max_length: 256

# beam
beam_size: 10
beam_length_penalty: 1.0
max_decoding_steps: 256
# rerun_ids: "save/ds-bart-wiki18/ds-merge-desc-pattent-2.0/errids-wqcwq-ver-1-penalty-1.0.json"

# optimizer
weight_decay: 1e-5
warmup_proportion: 0.1  # best: 1.6w-3w. 2219611 / 32 * 5 epoch * 0.1 = 3.4w
adam_epsilon: 1e-8
max_grad_norm: 1.0
alpha: 1e-4
learning_rate: 5e-5
num_gradient_accumulation_steps: 1

# train
num_epochs: 20
max_instances_train: null
train_batch_size: 64
factor_scheduler: 0.95
patience_scheduler: 1
patience_trainer: 3
cuda_index: 0
grad_clipping: 1.0

# test
test_batch_size: 0
max_instances_test: 0

# log
log_name: "log.json"